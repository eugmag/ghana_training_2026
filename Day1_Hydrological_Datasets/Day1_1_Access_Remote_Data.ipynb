{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Accessing Remote Hydroclimate Data**  \n",
        "\n",
        "Hydroclimate sciences increasingly rely on large, and frequently updated datasets. Observations, reanalyses, model simulations, and projections are often hosted on remote servers operated by national agencies, modelling centres, research institutions, or upcoming cloud platforms. Efficiently accessing these datasets is therefore a core technical skill for modern data-driven research, operational monitoring and climate services.\n",
        "\n",
        "With the latest generation of hydroclimate datasets traditional “download everything” workflows increasingly inefficient and, in many cases, impractical. As datasets grow, these limitations can become a bottleneck for both research and operational applications. Remote data access enables more efficient, collaborative, and reproducible scientific workflows.\n",
        "\n",
        "![](https://raw.githubusercontent.com/NERC-CEH/ghana_hydro_training_2026/refs/heads/main/content/remote_access_benefits.png)\n",
        "\n",
        "This notebook introduces the key concepts and practical methods for accessing large hydroclimate data from remote sources. Rather than focusing on a single tool or platform, it provides a framework for choosing the most appropriate data access strategy based on the data type, size, update frequency, and analysis goals. By working through this notebook, you will:\n",
        "\n",
        "- Understand **why remote data access is essential** for modern hydroclimate workflows  \n",
        "- Learn how to **choose between different data access methods**, including:\n",
        "\n",
        "![](https://raw.githubusercontent.com/NERC-CEH/ghana_hydro_training_2026/refs/heads/main/content/types_of_access.png)\n",
        "\n",
        "- Build **workflows for remote access using real-world datasets**, that can be applied for reproducible research and operational applications\n",
        "\n",
        "Choosing the **right data access method** depends on factors such as dataset size, format, update frequency, and whether full datasets or targeted subsets are required. Understanding these trade-offs helps you design efficient workflows and avoid common bottlenecks.Choosing the right data access method depends on the type of data and how you plan to use it. The table below provides simple guidance on when to use each access approach, followed by a flow chart that offers a quick decision framework to help you select the most suitable method.\n",
        "\n",
        "| Method | Subsetting | Automation | Best for |\n",
        "|------|-----------|------------|---------|\n",
        "| Manual download | ❌ | ❌ | Small, one-off datasets |\n",
        "| Command line | ❌ | ✔ | Bulk static files |\n",
        "| Data Access Protocols | ✔ | ✔ | NetCDF / gridded data |\n",
        "| APIs | ✔✔ | ✔✔ | Station & event data |\n",
        "| Cloud storage | ✔ | ✔✔ | Very large datasets |\n",
        "| GEE / Data cubes | ✔✔✔ | ✔✔✔ | Large-scale analysis |\n",
        "\n",
        "![](https://raw.githubusercontent.com/NERC-CEH/ghana_hydro_training_2026/refs/heads/main/content/remote_access_decision_tree.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "AxxqEdNDOZ9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Packages**"
      ],
      "metadata": {
        "id": "20Ul_1cGqD04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installing packages that are required above the ones already installed in Google Colab - ONLY NEEDS TO BE RUN IF USING GOOGLE COLAB\n",
        "!pip install zarr cftime s3fs netCDF4==1.6.0 cdsapi\n",
        "\n",
        "# If you are running this notebook in a platform other than Google Colab, please use the following command to install all the required packages. NOT REQUIRED FOR BINDER\n",
        "#!python -m pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "NzLU4_qCkzt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I. Download-based access**\n",
        "\n",
        "Download-based access refers to retrieving data files from remote servers and storing them locally before analysis. This approach is commonly used when datasets are provided as static files and when users require full access to the data contents. Download-based methods include manual downloads through web browsers as well as automated retrieval using command-line tools. While this approach is simple and widely supported, it can become inefficient for large datasets or repeated analyses, as it requires transferring and storing complete files locally. Nevertheless, download-based access remains a practical and important option for small to moderate data volumes, one-off analyses, and environments where remote subsetting or streaming is not available."
      ],
      "metadata": {
        "id": "nhfCgOpYOaK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Manual**\n",
        "\n",
        "### When should I use this?\n",
        "✔ Small number of files  \n",
        "✔ One-off analysis  \n",
        "✔ Human inspection needed  \n",
        "\n",
        "⚠ Not suitable for automation or large datasets\n",
        "\n",
        "Direct data download refers to the process of retrieving datasets from a remote server via a straightforward HTTP, HTTPS, or FTP/SFTP link. This method is commonly used when data are stored as static files (e.g., NetCDF, CSV, GRIB) and made accessible through a direct URL. Users can download files manually through a browser. While this approach is simple and widely supported, it may be less efficient for accessing large datasets or performing repeated queries on multiple files, in which case more advanced methods may be preferable. Some tools are available, such as [Filezilla](https://filezilla-project.org/), for accessing FTP/SFTP servers for bulk downloads.\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "https://environment.data.gov.uk/hydrology/explore\n",
        "\n",
        "https://portal.grdc.bafg.de/applications/public.html?publicuser=PublicUser#dataDownload/Home"
      ],
      "metadata": {
        "id": "BHafp7wXOaN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Automated tools**\n",
        "\n",
        "Tools are available, such as [Filezilla](https://filezilla-project.org/), for accessing FTP/SFTP servers for bulk downloads.\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "https://environment.data.gov.uk/hydrology/explore\n",
        "\n",
        "https://portal.grdc.bafg.de/applications/public.html?publicuser=PublicUser#dataDownload/Home"
      ],
      "metadata": {
        "id": "mzleM46qIeHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Command line**\n",
        "\n",
        "Datasets hosted on remote servers can be downloaded via HTTP, HTTPS, or FTP links using command-line tools in Linux. These methods are especially useful for automating bulk downloads from static URLs. With Linux shell scripting, such downloads can also be parallelised to efficiently handle multiple files simultaneously. This method downloads the full data available via the link being used, and most of the time cannot download subsets of one file.\n",
        "\n",
        "[Parallel computing](https://www.geeksforgeeks.org/computer-science-fundamentals/introduction-to-parallel-computing/) can significantly speed up the process of downloading large numbers of files from the command line. Instead of downloading files one at a time, multiple downloads can be done simultaneously, making efficient use of available CPU and network resources. This approach is especially useful when working with large datasets with multiple files or when accessing data from remote servers with high latency (i.e., delay in communication between your computer and the remote server).\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "https://catalogue.ceh.ac.uk/documents/dbf13dd5-90cd-457a-a986-f2f9dd97e93c\n",
        "\n",
        "https://www.ncei.noaa.gov/pub/data/"
      ],
      "metadata": {
        "id": "cYPUV3xVOaQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: The ! operator in Python-based Jupyter Notebooks allows users to execute Linux shell commands directly from within the notebook, effectively stepping out of the Python environment to run system-level commands.*"
      ],
      "metadata": {
        "id": "OX3DiOgmbhJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i. Downloading single file"
      ],
      "metadata": {
        "id": "RZyYjm1pbitB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://catalogue.ceh.ac.uk/datastore/eidchub/dbf13dd5-90cd-457a-a986-f2f9dd97e93c/GB/monthly/CEH_GEAR_monthly_GB_1894.nc"
      ],
      "metadata": {
        "id": "7NWu3VvFbYn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For now we are deleting the data\n",
        "!rm CEH_GEAR_monthly_GB_1894.nc"
      ],
      "metadata": {
        "id": "EDxZf2GgbuSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii. Downloading restricted data"
      ],
      "metadata": {
        "id": "pvV-eHKybsBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# To use wget for multiple files or even ftp servers which are linked to your account and password protected you can use the following version\n",
        "# For detailed example of accessing servers with password protection please see: https://eidc.ac.uk/help/getdata/downloadData\n",
        "# Add your own username and password to download the whole catalogue.\n",
        "# Please do not try this during the training session as it would take a lot time and storage space.\n",
        "!wget --user=YOUR_USERNAME --password=YOUR_PASSWORD --auth-no-challenge https://catalogue.ceh.ac.uk/datastore/eidchub/dbf13dd5-90cd-457a-a986-f2f9dd97e93c\n",
        "'''"
      ],
      "metadata": {
        "id": "yJaKan4WbYlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii. Downloading multiple files"
      ],
      "metadata": {
        "id": "cpdPg4nJcF6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Multiple files can be available to us sometimes in compressed format like the following\n",
        "!wget https://www.ncei.noaa.gov/pub/data/hourly_precip-3240/01/3240_01_1948-1998.tar.Z\n",
        "'''"
      ],
      "metadata": {
        "id": "NfOe9KA-bYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "%%capture\n",
        "# There are command line methods to uncompress the datasets\n",
        "!tar -zxvf 3240_01_1948-1998.tar.Z\n",
        "'''"
      ],
      "metadata": {
        "id": "SOdTHLnJbYeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# For now we are deleting the data\n",
        "!rm 3240_*\n",
        "'''"
      ],
      "metadata": {
        "id": "0hQoejZBbYU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# To download multiple files via wget, you can use a txt file that has a list of the URLs\n",
        "# First as an example we create a txt file with urls we want to download\n",
        "url_list = ['https://www.ncei.noaa.gov/pub/data/daily-grids/beta/by-month/1951/01/prcp-195101-cen-scaled.csv',\n",
        "            'https://www.ncei.noaa.gov/pub/data/daily-grids/beta/by-month/1952/01/prcp-195201-cen-scaled.csv']\n",
        "\n",
        "with open(\"urls.txt\", \"w\") as outfile:\n",
        "    outfile.write(\"\\n\".join(url_list))\n",
        "\n",
        "!more urls.txt\n",
        "'''"
      ],
      "metadata": {
        "id": "WoomYtUhcR1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Then use the text file with the URL list to download multiple files\n",
        "!wget -i urls.txt\n",
        "'''"
      ],
      "metadata": {
        "id": "-Fg_n-uvcTMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# For now we are deleting the data\n",
        "!rm prcp* urls.txt\n",
        "'''"
      ],
      "metadata": {
        "id": "CGcupBDCcS_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II. Access without downloading**\n",
        "\n",
        "Hydroclimate data can be accessed remotely without downloading entire datasets. Multiple platforms allow users to query, subset, and stream data directly into analysis environments such as Python, R, or MATLAB. Some methods, such as APIs and FTP/HTTP access, support dynamic data slicing using query parameters. However, remote data access more broadly enables scalable, on-demand computing by allowing users to process and analyze data without the need for local storage. Such remote access methods are increasingly critical for handling the growing volume of high-resolution climate data."
      ],
      "metadata": {
        "id": "lx2r7GnWOaTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Access Protocols/Servers**\n",
        "\n",
        "These are servers that mostly designed to host/access large multidimensional scientific datasets (e.g., NetCDF or HDF formats) but are not suitable for other formats like CSV or JSON. Some types of these servers are: THREDDS (Thematic Real-time Environmental Distributed Data Services); OPeNDAP (Open-source Project for a Network Data Access Protocol). These can be accessed via scientific tools/libraries (e.g., xarray, netCDF4, nccopy, Panoply) or protocols like OPeNDAP allow client-side access via URLs (wget). There is some flexibility for accessing subset of data but no functionality for filtering by metadata or aggregation.\n",
        "\n",
        "Some other resources for learning how to use such type of servers are available at [Galaxy Climate Workbench](https://nordicesmhub.github.io/Norway_Sweden_training/pangeo/search_and_load_with_esgf_opendap.html), [eScience tools in Climate Science](https://nordicesmhub.github.io/forces-2021/learning/example-notebooks/search_and_load_with_esgf_opendap.html), [Project Pythia](https://projectpythia.org/cmip6-cookbook/notebooks/foundations/esgf-opendap/), and [University of Toronto Climate Downscaling Workflow](https://utcdw.physics.utoronto.ca/UTCDW_Guidebook/Chapter3/section3.4_climate_model_output.html)\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "https://www.nccs.nasa.gov/services/data-collections/land-based-products/nex-gddp-cmip6\n",
        "\n",
        "https://ds.nccs.nasa.gov/thredds/catalog/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/catalog.html\n",
        "\n",
        "https://ds.nccs.nasa.gov/thredds/catalog/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/catalog.html?dataset=AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v1.1.nc\n",
        "\n",
        "For the training session we will be accessing [CMIP6](https://pcmdi.github.io/CMIP6/) simulations for precipitation from [sixth version of the Model for Interdisciplinary Research on Climate (MIROC6)](https://doi.org/10.5194/gmd-12-2727-2019) for the historial period year 1950."
      ],
      "metadata": {
        "id": "JLoeBhKZOaWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i. Reading in single file"
      ],
      "metadata": {
        "id": "yt2ApLAvoy-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing packages\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ZCYN6z2TOFji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use the \"OpenDAP\" URL to directly load data into Python\n",
        "# https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v2.0.nc.html\n",
        "f = xr.open_dataset('https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v2.0.nc')\n",
        "f"
      ],
      "metadata": {
        "id": "3aZC1QjHc-_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The OpenDAP servers provide the functionality to subset the data as needed\n",
        "f = xr.open_dataset(\"https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v2.0.nc?lat[0:1:100],pr,time[0:1:10],lon[0:1:100]\")\n",
        "f"
      ],
      "metadata": {
        "id": "eo-T2cAEd8Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or you can read the full data file and subset it using xarray\n",
        "f = xr.open_dataset('https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_1950_v2.0.nc')\n",
        "f.sel(lat=slice(-40,0), lon=slice(10,50)).pr[0].plot()"
      ],
      "metadata": {
        "id": "qCBb9tdHd8KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii. Reading in multiple files together"
      ],
      "metadata": {
        "id": "z35CF_Ccc_QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Using the \"OpenDAP\" URLs for different files, you can read multiple files together without downloading\n",
        "\n",
        "# For this you would need to first create a list of the file URLs\n",
        "base_url = 'https://ds.nccs.nasa.gov/thredds/dodsC/AMES/NEX/GDDP-CMIP6/MIROC6/historical/r1i1p1f1/pr/pr_day_MIROC6_historical_r1i1p1f1_gn_'\n",
        "years = range(1950, 1952)  # Adjust the range as needed\n",
        "\n",
        "file_urls = [f\"{base_url}{year}_v2.0.nc\" for year in years]\n",
        "file_urls\n",
        "'''"
      ],
      "metadata": {
        "id": "LxK7fHwNc_gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Then use the list of URLs to open all of them directly using xarray function to open multiple files\n",
        "f = xr.open_mfdataset(file_urls, combine='by_coords')\n",
        "f.sel(lat=slice(0,40), lon=slice(-20,20))\n",
        "'''"
      ],
      "metadata": {
        "id": "_R0Nxo8HeJLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. APIs (Application Programming Interfaces)**\n",
        "\n",
        "Now a days, many data providers offer RESTful APIs that allow users to query and retrieve data programmatically. APIs are powerful for accessing dynamic content, filtering by time, location, or variable, and automating data workflows. The filtering can easily be done using Python scripts.\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "[Climate Data Store](https://cds.climate.copernicus.eu/datasets/derived-era5-single-levels-daily-statistics?tab=overview): Some example code showing the direct use of reanalysis to analyse [heatwaves](https://colab.research.google.com/github/ecmwf-projects/copernicus-training-c3s/blob/main/reanalysis-heatwave.ipynb#scrollTo=2BAgk0Coobqt) and [climatology](https://colab.research.google.com/github/ecmwf-projects/copernicus-training-c3s/blob/main/reanalysis-climatology.ipynb#scrollTo=jkz0LhSbK5K9). Please click here for more about how to set up [CDS API](https://cds.climate.copernicus.eu/how-to-api).\n",
        "\n",
        "[COSMOS API](https://cosmos-api.ceh.ac.uk/docs): Some example code available at\n",
        "[COSMOS Data Access Python Snippets](https://cosmos-api.ceh.ac.uk/python_examples)\n",
        "\n",
        "For the training session we will be accessing [CDS ERA5 reanalysis](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=download). We will be accessing observed variable of 2m Air Temperature (T2M) for 01-Jan-2000 for the wider West Africa region, directly from the API. For this you will need to have an [ECMWF registered account](https://accounts.ecmwf.int/auth/realms/ecmwf/login-actions/registration?client_id=cds&tab_id=A8nOgKYE6EI)."
      ],
      "metadata": {
        "id": "rk2xUUM6OaYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import cdsapi\n",
        "\n",
        "URL = 'url: https://cds.climate.copernicus.eu/api'\n",
        "KEY = 'key: 8f188f32-8ee7-4dc0-bded-134be9adae38'\n",
        "\n",
        "with open('/root/.cdsapirc', 'w') as f:\n",
        "    f.write('\\n'.join([URL, KEY]))\n",
        "\n",
        "#with open('/root/.cdsapirc') as f:\n",
        "#    print(f.read())"
      ],
      "metadata": {
        "id": "XtyaR81UqTts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = cdsapi.Client()\n",
        "c.retrieve(\"reanalysis-era5-single-levels\",\n",
        "    {\n",
        "    \"product_type\": [\"reanalysis\"],\n",
        "    \"variable\": [\"2m_temperature\"],\n",
        "    \"year\": [\"2000\"],\n",
        "    \"month\": [\"01\"],\n",
        "    \"day\": [\"12\"],\n",
        "    \"time\": [\"00:00\"],\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\",\n",
        "    \"area\": [40, -20, 0, 20]\n",
        "    },\n",
        "           target = \"era5_temp.nc\")"
      ],
      "metadata": {
        "id": "bKUptmkvmJTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "era5 = xr.open_dataset(\"era5_temp.nc\")\n",
        "era5"
      ],
      "metadata": {
        "id": "mnU-sPiqqyUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "era5['t2m'].plot()"
      ],
      "metadata": {
        "id": "fItfgbg7Lamj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm era5_temp.nc"
      ],
      "metadata": {
        "id": "ge7xkiq2LBme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Cloud storage**\n",
        "\n",
        "Many datasets are now being hosted in cloud storages, especially for large-scale earth observation data. Tools like AWS CLI, boto3 (Python), gsutil, or cloud-native file systems (e.g., s3fs, gcsfs) allow seamless access to cloud-hosted data. Examples are AWS S3, Google Cloud Storage, Azure. In this tutorial we will be looking at the object store type of cloud storage data.\n",
        "\n",
        "**What is Object Storage?**\n",
        "\n",
        "Object storage is a cloud-native technology specifically designed for efficiently managing large unstructured datasets. While object store offers significant cost-effectiveness compared to disk storage, it requires data to be appropriately adapted to fully realise its benefits. Data retrieval from object store is over HTTP in complete \"objects\", which are either entire files or file chunks. This is different to the traditional file storage:\n",
        "\n",
        "![](https://raw.githubusercontent.com/NERC-CEH/ghana_hydro_training_2026/refs/heads/main/content/object_vs_file_storage.png)\n",
        "\n",
        "As this is relatively new technology, there is a clear lack of established tools and best-practice for converting file types for optimal use with object storage, particularly for large gridded and N-dimensional datasets used in environmental and climate sciences. The performance and speed of object storage are contingent upon the data's structure, chunking, and the specific analysis requirements of the user. Consequently, a better understanding of these interactions is essential before widespread adoption.\n",
        "\n",
        "***Demonstration examples:***\n",
        "\n",
        "For the training session we will be accessing [CHESS-SCAPE data](https://dx.doi.org/10.5285/8194b416cbee482b89e0dfbe17c5786c), which has been stored in the [JASMIN Object Store](https://help.jasmin.ac.uk/docs/short-term-project-storage/using-the-jasmin-object-store/). We will extract the maximum daily air temperature (TASMAX) from four different ensemble members at the grid point closest to the Alice Holt station, and compare the CHESS-SCAPE data with COSMOS observational data.\n",
        "\n",
        "**Other cloud-based Infrastructure**\n",
        "\n",
        "There are several other ways to access data remotely, and increasingly, most of these methods are being supported by cloud-based infrastructure.  \n",
        "\n",
        "  - Emerging cloud-native standards, such as the Spatiotemporal Asset Catalog (STAC), enable efficient cataloging and discovery of satellite or gridded climate data across distributed systems. These are often integrated with cloud platforms, allowing users to search and access data programmatically with minimal overhead. For example, [ECMWF Data Stores STAC Catalogue API](https://cds.climate.copernicus.eu/stac-browser/?.language=en).\n",
        "  - Sensor Observation Services (SOS) are also available and are particularly useful for accessing real-time or near-real-time observations, especially from in-situ sensor networks or environmental monitoring platforms. For example, [UK AIR](https://uk-air.defra.gov.uk/data/data-availability). In the near future, the [FDRI project](https://fdri.org.uk/) will provide monitoring data covering the whole hydrological system.\n",
        "  - Finally, platforms such as [Google Earth Engine (GEE)](https://developers.google.com/earth-engine/datasets) and other open data cube frameworks enable users to query and analyze massive gridded datasets remotely, leveraging built-in computational resources—eliminating the need to download data locally. The GEE platform offers [comprehensive tutorials](https://developers.google.com/earth-engine/guides/getstarted) that guide users through its functionality. As a self-learning exercise, we have included a GitHub repository developed at UKCEH, linked in this workshop’s directory. It provides step-by-step training on [extracting drought indicators using GEE in Python](https://github.com/eugmag/Google_Earth_Engine_python_demo/tree/main), allowing you to follow along at your own pace.\n",
        "\n",
        "**OTHER RESOURCES:**\n",
        "1. To hear more details for object storage please see the video on [JASMIN Object Storage: Optimizing Performance for Climate Research](https://www.youtube.com/watch?v=xJ8qEXQAri0&list=PLhF74YhqhjqnXvjzFCKnw4TGAFnkVu7Qn&index=2) from the JASMIN User Conference 2023.\n",
        "2. As a self-learning exercise, we have included a [GitHub respository](https://github.com/NERC-CEH/object_store_tutorial) developed at UKCEH, linked in this workshop’s directory. It provides notebooks and guides on utilizing object storage.\n"
      ],
      "metadata": {
        "id": "jPiQj_wDOabs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i. Importing required packages and pre-written functions"
      ],
      "metadata": {
        "id": "pqHLP0N8jiXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fsspec\n",
        "import s3fs\n",
        "import zarr\n",
        "import xarray as xr"
      ],
      "metadata": {
        "id": "lNvWbvEGjhxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_zarr_from_s3(endpoint_url: str,\n",
        "                      store_path: str):\n",
        "    \"\"\"\n",
        "    Open a Zarr dataset hosted on an S3‑compatible object store.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    endpoint_url : str\n",
        "        Base S3 endpoint, e.g. \"https://chess-scape-o.s3-ext.jc.rl.ac.uk\".\n",
        "    store_path : str\n",
        "        Path to the Zarr store inside the bucket, e.g.\n",
        "        \"ens01-year100kmchunk/tmax_01_year100km.zarr\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xr.Dataset\n",
        "        The opened Zarr dataset.\n",
        "    \"\"\"\n",
        "    # 1. Create an fsspec filesystem for the S3 endpoint\n",
        "    fs = fsspec.filesystem(\"s3\", asynchronous=True, anon=True,\n",
        "                           endpoint_url=endpoint_url,)\n",
        "\n",
        "    # 2. Wrap it in a Zarr store\n",
        "    zstore = zarr.storage.FsspecStore(fs, path=store_path)\n",
        "\n",
        "    # 3. Open the dataset with xarray\n",
        "    ds = xr.open_zarr(zstore, decode_times=True, decode_cf=True)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "FUrDdeF9jrH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii. Exploring the data in Object Store"
      ],
      "metadata": {
        "id": "4htThAH9jkOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JASMIN Object Store tenancy we are using is chess-scape-o, the URL is as follows\n",
        "jasmin_s3_url = \"https://chess-scape-o.s3-ext.jc.rl.ac.uk\""
      ],
      "metadata": {
        "id": "NDfu-u5gjkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s3fs is a python package that allows you to not only read the data but also explore the tenancy ()\n",
        "# Here we will be using s3fs to list the bucket and not read the data, we read the data using intake package shown below\n",
        "# For more information please see: https://pypi.org/project/s3fs/\n",
        "s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'endpoint_url': jasmin_s3_url})\n",
        "s3.ls('s3://ens01-year100kmchunk/')\n",
        "\n",
        "# In the output you see that within in the chess-scape-o tenancy, a bucket called ens01-year100kmchunk\n",
        "# has 10 different zarr files for different 10 different variables. This is for a single chunk type tested."
      ],
      "metadata": {
        "id": "k15U9j_Jj5-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii. Accessing data and the associated metadata"
      ],
      "metadata": {
        "id": "inQmr0WHj8Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are accessing TASMAX for the ensemble member #01 from the catalogue\n",
        "chess_data = open_zarr_from_s3(jasmin_s3_url, \"ens01-year100kmchunk/tmax_01_year100km.zarr\")\n",
        "chess_data"
      ],
      "metadata": {
        "id": "n3kUMzlFj7E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHESS-SCAPE is on the British National Grid with Easting and Northing Coordinates.\n",
        "# We also set the latitude and longitude as coordinates\n",
        "chess_data = chess_data.set_coords((\"lat\", \"lon\"))\n",
        "chess_data"
      ],
      "metadata": {
        "id": "gOdJBkVgj695"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing for the time period 2016--2022\n",
        "chess_data = chess_data[\"tasmax\"].sel(time=\"2016-01-01\")\n",
        "chess_data"
      ],
      "metadata": {
        "id": "LuWaV3Gyj6wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chess_data.plot()"
      ],
      "metadata": {
        "id": "V5KzpKmVkHVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "Modern hydroclimate datasets are increasingly large, distributed, and dynamic, requiring data access approaches that go beyond manual downloading. Remote data access enables more efficient, scalable, and reproducible workflows by allowing users to retrieve, subset, or stream data directly from where it is hosted.\n",
        "\n",
        "**Together, these characteristics motivate a shift from manual downloading toward remote, programmatic, and scalable data access methods.**\n",
        "\n",
        "There is no single “best” method for accessing remote data. The appropriate approach depends on the size and structure of the dataset, how frequently it is updated, and whether full datasets or targeted subsets are required. Manual and command-line downloads remain useful for small or static datasets, while scientific data servers, APIs, and cloud-native platforms are better suited for large, multidimensional, or frequently updated data.\n",
        "\n",
        "Understanding the distinction between data access protocols and scientific data servers helps clarify how remote access works in practice and enables users to choose suitable tools with confidence. By applying a simple decision framework, users can design workflows that are efficient, automated, and adaptable across different data sources and computing environments.\n",
        "\n",
        "The concepts and examples in this notebook provide a foundation for accessing a wide range of hydrological and hydroclimate datasets and can be readily extended to support research, operational monitoring, and climate services applications.\n",
        "\n",
        "\n",
        "#### ***Thank you! Come talk to us about what sort of data are you looking to download and/or access?***"
      ],
      "metadata": {
        "id": "SVb-7-q5Oahc"
      }
    }
  ]
}
