{"cells":[{"cell_type":"markdown","id":"b95c92a6-3778-470e-acd5-1dbb53e7d0c8","metadata":{"id":"b95c92a6-3778-470e-acd5-1dbb53e7d0c8"},"source":["# Part 1 - Textual Data"]},{"cell_type":"markdown","id":"8790039b-3ad1-4939-87d0-fd30a6fc8e76","metadata":{"id":"8790039b-3ad1-4939-87d0-fd30a6fc8e76"},"source":["In this workshop, we will provide an introduction to working with the various types of hydrological data you are likely to encounter.\n","\n","The learning objectives are to:\n","\n","- Learn how to load various types of data from object storage (\"the cloud\")\n","- Learn about the different types of textual file formats that are commonly used and how to work with them in Python using Pandas and Numpy\n","- Learn about the different types of spatial file formats that are commonly used and how to work with them in Python using GeoPandas\n","- Learn about the new Zarr format for multi-dimensional large datasets and how to work with it in Python using Xarray\n","- Learn how to utilise Matplotlib and Cartopy's plotting libraries to quickly visualise data\n","\n","We will be teaching via Jupyter notebooks - which you are hopefully reading this from. We will be going through the notebooks cell-by-cell together. Code cells can be run individually by clicking on the cell and using ```Shift+Enter```.\n","\n","There will be three notebooks covering, respectively, textual data, vector data, gridded or multi-dimensional data.\n"]},{"cell_type":"markdown","id":"a939219e-d698-4091-b6c2-7d11d38bf658","metadata":{"id":"a939219e-d698-4091-b6c2-7d11d38bf658"},"source":["## Textual data"]},{"cell_type":"markdown","id":"aa6c3b1e-c58d-4fcd-a2df-9ef5c8d7ec3b","metadata":{"id":"aa6c3b1e-c58d-4fcd-a2df-9ef5c8d7ec3b"},"source":["In this, first, notebook we will focus on textual, human-readable data.\n","\n","Before getting started we need to install and import the packages that we will be using in the notebook."]},{"cell_type":"code","execution_count":null,"id":"b39db0c2-5680-465b-8ead-43164a0e7884","metadata":{"id":"b39db0c2-5680-465b-8ead-43164a0e7884"},"outputs":[],"source":["## ONLY NEEDS TO BE RUN IF USING GOOGLE COLABS\n","%%capture\n","!pip install s3fs"]},{"cell_type":"code","execution_count":null,"id":"a70e382d-574a-4b7e-95b7-54b1c6691501","metadata":{"id":"a70e382d-574a-4b7e-95b7-54b1c6691501"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xarray as xr\n","import matplotlib.pyplot as plt\n","import s3fs\n","from IPython.display import Image"]},{"cell_type":"markdown","id":"d86e0df6-8643-4768-a80b-86e65c0744f3","metadata":{"id":"d86e0df6-8643-4768-a80b-86e65c0744f3"},"source":["We will be using data stored in an object storage bucket through out the session (see the [Data Access notebook](https://github.com/eugmag/ghana_training_2026/blob/main/Day1_Hydrological-Datasets/Ghana_Remote_Access_training.ipynb) for a reminder of how to make use of remote storage):"]},{"cell_type":"code","execution_count":null,"id":"3b270315-c5a4-4b49-ba10-9c127ed8bd77","metadata":{"scrolled":true,"id":"3b270315-c5a4-4b49-ba10-9c127ed8bd77"},"outputs":[],"source":["s3 = s3fs.S3FileSystem(anon=True, client_kwargs={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\"})\n","s3.ls('s3://example-data/')"]},{"cell_type":"markdown","id":"8d9286e5-6ba0-4860-9be3-2b9a5408c591","metadata":{"id":"8d9286e5-6ba0-4860-9be3-2b9a5408c591"},"source":["Often referred to as '[ASCII](https://en.wikipedia.org/wiki/ASCII)', textual data is data that is human-readable, this means that you can open it with a text editor and see the actual data written down. This differs from binary data formats - like NetCDF, Zarr, Shapefiles - which are stored directly as specifically laid-out binary digits - 0s and 1s - which only computers can read directly. Exactly how the 0s and 1s are laid out and exactly how they represent/translate to the data we are interested in (the encoding) defines the file format. They need special programmes that understand the layout and encoding in order to be read in and processed.\n","\n","Textual data is considered by many to be easier to work with as all you need is a text editor to get started. Data that is typically stored as ASCII includes gridded outputs from some hydrological models and rain-gauge data.\n","\n","We will be using [Pandas](https://pandas.pydata.org/), a powerful python library for working with tabulated data, which is what you usually get in textual data files. We will explore some of what you can do with Pandas here, but there is a huge amount of functionality we won't cover that is documented on the Pandas website.\n","\n","We will also be reading in data that is stored remotely on object storage instead of locally on disk."]},{"cell_type":"markdown","id":"a973f120-4622-4f75-a7a1-56465ea16bc4","metadata":{"id":"a973f120-4622-4f75-a7a1-56465ea16bc4"},"source":["Let's start with some simple textual river flow data, such as that which might be output by a hydrological model:"]},{"cell_type":"code","execution_count":null,"id":"b8c403fc-0ed7-46ff-88aa-c4578f14ad42","metadata":{"id":"b8c403fc-0ed7-46ff-88aa-c4578f14ad42"},"outputs":[],"source":["obsdata = pd.read_csv('s3://example-data/obsflows.csv', storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})\n","\n","obsdata"]},{"cell_type":"markdown","id":"77f8a016-9ca7-4375-b564-2eb1dfb00d4f","metadata":{"id":"77f8a016-9ca7-4375-b564-2eb1dfb00d4f"},"source":["This is **csv** data. CSV stands for \"[Comma Separated Values](https://en.wikipedia.org/wiki/Comma-separated_values)\" and means that if you were to open this data up in a text editor you would see the values separated by commas. In this case the commas are the \"delimeters\" that separate or \"delimit\" the individual values from each other. Even though CSV stands for **Comma** Separated Values, you may also see other characters being used as delimiters in CSV files, the most common alternative is tabs, which show up as gaps between the numbers in a text editor."]},{"cell_type":"markdown","id":"039c87b8-d5c8-40f6-a4bd-0f5405a497f0","metadata":{"id":"039c87b8-d5c8-40f6-a4bd-0f5405a497f0"},"source":["Taking a look at the data table now, we can see that we have rows representing the date and it's not immediately clear what the columns represent. The column headers are in fact catchment IDs, a 5-digit number given to all catchments in the UK, with the first two numbers representing the river-basin the catchment ultimately drains into and the last 3 numbers are zero-padded and are representing the sub-catchment within this river basin. So 39001 translates as 'river basin number 39' (which is the Thames) and sub-catchment number '01' (which in this case is the Thames as far as Kingston). More information on how catchments are identified here: https://nrfa.ceh.ac.uk/data/about-data/catchment-information"]},{"cell_type":"markdown","id":"f74980f5-d450-4ac3-a43f-2bd712af8362","metadata":{"id":"f74980f5-d450-4ac3-a43f-2bd712af8362"},"source":["This is flow data, so each value in the table will be a flow amount in $m^3/day$. We can see straight away that there are a lot of -1 values. This is likely missing data."]},{"cell_type":"markdown","id":"2cb28376-5eb8-423b-8dc6-fd105db888ac","metadata":{"id":"2cb28376-5eb8-423b-8dc6-fd105db888ac"},"source":["We can also see that the date information is split over three columns (day, month, and year). Pandas has some excellent date-time handling capabilities, but it sometimes needs to be told how to interpret the date-time information in the data first before this can be unlocked.\n","\n","We can use pandas to read in the CSV again and tell it which columns to convert to a date."]},{"cell_type":"code","execution_count":null,"id":"0c5a33e4-cc8d-4428-a110-05c88939ed7d","metadata":{"id":"0c5a33e4-cc8d-4428-a110-05c88939ed7d"},"outputs":[],"source":["%%capture\n","obsdata = pd.read_csv('s3://example-data/obsflows.csv', # read in the csv\n","                      parse_dates={'Times': [0,1,2]}, dayfirst=True,  # use the first three colums to create the date\n","                      storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})\n","obsdata.set_index('Times', inplace=True) # set the \"Times\" column as index"]},{"cell_type":"markdown","id":"1f8f787b-0d56-4815-9ae2-d812935e4dcc","metadata":{"id":"1f8f787b-0d56-4815-9ae2-d812935e4dcc"},"source":["The first line is where we read in the data, note we have now added in an extra argument telling pandas how to handle the date-time information in the data."]},{"cell_type":"markdown","id":"f4deff92-7e0a-41bd-af31-f81be045bdb8","metadata":{"id":"f4deff92-7e0a-41bd-af31-f81be045bdb8"},"source":["The second line sets the new 'Times' column that we have created from the original 'day', 'month', and 'year' columns as the Index, which is what Pandas uses to work with the data."]},{"cell_type":"code","execution_count":null,"id":"dcd1550e-8eae-4297-9338-3405c7e20858","metadata":{"id":"dcd1550e-8eae-4297-9338-3405c7e20858"},"outputs":[],"source":["obsdata"]},{"cell_type":"markdown","id":"f824305b-08c1-4fab-8183-a56b57f697c6","metadata":{"id":"f824305b-08c1-4fab-8183-a56b57f697c6"},"source":["Let's now have a look at one of the catchments in detail:"]},{"cell_type":"markdown","id":"1cf3a223-b95e-4ef4-8ce7-1e10144b4da0","metadata":{"id":"1cf3a223-b95e-4ef4-8ce7-1e10144b4da0"},"source":["We can cleverly select out the date range and catchment we wish to plot using the row and column names. As the row names (or the 'Index' in Pandas parlance) contains labels for each row in YYYY-MM-DD format, we can use these names to select out individual or ranges of rows as ```dataset.loc[rowname, colname]``` or ```dataset.loc[startrowname:endrowname, startcolname:endcolname]```"]},{"cell_type":"code","execution_count":null,"id":"7074c417-0038-43d1-9e01-cc82795ff531","metadata":{"id":"7074c417-0038-43d1-9e01-cc82795ff531"},"outputs":[],"source":["obsplot = obsdata.loc['2000-01-01':'2015-12-31', '39010']"]},{"cell_type":"markdown","id":"b3924820-6633-4da7-b831-b417b0aa0cee","metadata":{"id":"b3924820-6633-4da7-b831-b417b0aa0cee"},"source":["Pandas integrates with the [matplotlib](https://matplotlib.org/) plotting library directly, and you can just call ```.plot()``` on a dataframe to get a simple visual representation of the data:"]},{"cell_type":"code","execution_count":null,"id":"ad41fb31-71dd-4672-bbac-12881ab29d8a","metadata":{"id":"ad41fb31-71dd-4672-bbac-12881ab29d8a"},"outputs":[],"source":["obsplot.plot()"]},{"cell_type":"markdown","id":"0af5790a-e9b8-4a18-9996-d975eb744d34","metadata":{"id":"0af5790a-e9b8-4a18-9996-d975eb744d34"},"source":["If we wish to modify the plot, this is also possible:"]},{"cell_type":"code","execution_count":null,"id":"aec49924-0c44-4b66-9818-bd608cd27a6b","metadata":{"id":"aec49924-0c44-4b66-9818-bd608cd27a6b"},"outputs":[],"source":["plt.rcParams.update({'font.size': 10}) # make the text in the plot easier to read\n","ax = plt.gca() # gca = get current axis, allows us to get the most recent axis used...\n","obsplot.plot(ax=ax, label = 'obsflows') # ... and specify it to the plotting command, which is simply dataframename.plot()\n","ax.set_ylim([0, 25]) # set the y axis lower and upper bounds\n","plt.title('39010') # set the title\n","plt.ylabel('Daily mean flow (' + r'$m^3$' + ')') # set the y axis label\n","plt.legend() # add a legend"]},{"cell_type":"markdown","id":"4562924b-88e4-41ad-a999-d0c7ed0c0ae8","metadata":{"id":"4562924b-88e4-41ad-a999-d0c7ed0c0ae8"},"source":["Some of the data points look a bit unusual, suddenly going very low."]},{"cell_type":"markdown","id":"ee86a31b-f589-4693-8fd2-8b4f2d7b5db5","metadata":{"id":"ee86a31b-f589-4693-8fd2-8b4f2d7b5db5"},"source":["It's likely the '-1' values we could see in the table earlier. We can deal with these by telling pandas that this value represents a missing value, so that it ignores them when plotting them and doing any other analysis.\n","\n","We can tell pandas how to handle this by adding another argument to the ```.read_csv(...)``` function."]},{"cell_type":"code","execution_count":null,"id":"6a292747-e853-4d68-8f43-d2c93f103686","metadata":{"id":"6a292747-e853-4d68-8f43-d2c93f103686"},"outputs":[],"source":["%%capture\n","obsdata = pd.read_csv('s3://example-data/obsflows.csv', # read in the csv\n","                      parse_dates={'Times': [0,1,2]}, dayfirst=True, # use the first three colums to create the date\n","                      na_values = -1, # tell pandas that -1 represents a missing value\n","                      storage_options={'endpoint_url': \"https://fdri-o.s3-ext.jc.rl.ac.uk\", 'anon': True})\n","obsdata.set_index('Times', inplace=True) # set the \"Times\" column as index"]},{"cell_type":"code","execution_count":null,"id":"eae889e9-f91f-4856-b453-a22f4b80a142","metadata":{"id":"eae889e9-f91f-4856-b453-a22f4b80a142"},"outputs":[],"source":["obsdata"]},{"cell_type":"markdown","id":"b2245875-44db-4f2a-b237-b84ba5b49fae","metadata":{"id":"b2245875-44db-4f2a-b237-b84ba5b49fae"},"source":["We can see from the table above the **-1** values have been replace with **NaN**.\n","\n","Let's try the plotting again:"]},{"cell_type":"code","execution_count":null,"id":"2b81c0ea-64e1-4307-99de-9f80c118288e","metadata":{"id":"2b81c0ea-64e1-4307-99de-9f80c118288e"},"outputs":[],"source":["# timeperiod and gauge ID to plot\n","startplot = '2000-01-01'\n","endplot   = '2015-12-31'\n","catchmentplot = '39010'\n","\n","obsplot = obsdata[startplot:endplot]\n","\n","plt.rcParams.update({'font.size': 10})\n","ax = plt.gca()\n","obsplot[catchmentplot].plot(ax=ax, label = 'obsflows')\n","ylims = ax.get_ylim()\n","ax.set_ylim([0, ylims[1]])\n","plt.title(catchmentplot)\n","plt.ylabel('Daily mean flow (' + r'$m^3$' + ')')\n","plt.legend()"]},{"cell_type":"markdown","id":"88a6514d-7a5d-4ff8-9807-1de3dd938e80","metadata":{"id":"88a6514d-7a5d-4ff8-9807-1de3dd938e80"},"source":["Much better! What pandas has done is simply leave a blank space where the data is NaN, which would show up if you zoomed in the plot a bit more. We can zoom in on the plot by using the same plotting code as before but with a smaller time period:"]},{"cell_type":"code","execution_count":null,"id":"a58be9e3-d4fb-4ec8-8112-ef792b7f36ff","metadata":{"id":"a58be9e3-d4fb-4ec8-8112-ef792b7f36ff"},"outputs":[],"source":["# timeperiod and gauge ID to plot\n","startplot = '2010-01-01'\n","endplot   = '2010-05-31'\n","catchmentplot = '39010'\n","\n","obsplot = obsdata[startplot:endplot]\n","\n","plt.rcParams.update({'font.size': 10})\n","ax = plt.gca()\n","obsplot[catchmentplot].plot(ax=ax, label = 'obsflows')\n","ylims = ax.get_ylim()\n","ax.set_ylim([0, ylims[1]])\n","plt.title(catchmentplot)\n","plt.ylabel('Daily mean flow (' + r'$m^3$' + ')')\n","plt.legend()"]},{"cell_type":"markdown","source":["We can see that there is a small gap in the plot, indicating that pandas is correctly recognising the missing values!\n","\n","The same plotting code can be used for other catchments and time periods\n","\n","Use the snippet below to try plotting different catchments and time periods from the table.\n","\n","Some additional interesting catchment IDs:\n","- 39001 (Thames at Kingston)\n","- 39020 (Coln at Bibury)\n","- 81002 (Cree at Newton Stewart)\n","\n","Are there any differences between the flow patterns?\n","\n","The UK experienced notable hydrological events in 1975-1976 and 2010-2012. Can you plot these events?"],"metadata":{"id":"wvhnR9X1Bn-k"},"id":"wvhnR9X1Bn-k"},{"cell_type":"code","source":["# timeperiod and gauge ID to plot\n","startplot = '2005-01-01'\n","endplot   = '2008-12-31'\n","catchmentplot = '39020'\n","\n","obsplot = obsdata[startplot:endplot]\n","\n","plt.rcParams.update({'font.size': 10})\n","ax = plt.gca()\n","obsplot[catchmentplot].plot(ax=ax, label = 'obsflows')\n","ylims = ax.get_ylim()\n","ax.set_ylim([0, ylims[1]])\n","plt.title(catchmentplot)\n","plt.ylabel('Daily mean flow (' + r'$m^3$' + ')')\n","plt.legend()"],"metadata":{"id":"d3LaROl0K3Mk"},"id":"d3LaROl0K3Mk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Supplementary"],"metadata":{"id":"xNJA3kYYM_8l"},"id":"xNJA3kYYM_8l"},{"cell_type":"markdown","id":"c8af8d6d-953b-4b77-b750-fd3165c3f6af","metadata":{"id":"c8af8d6d-953b-4b77-b750-fd3165c3f6af"},"source":["Let's have a look at annual averages for a given catchments to see if we can pick out any particularly wet/dry years."]},{"cell_type":"markdown","id":"e07e606d-930e-4371-ac83-ba2e7b4f19b2","metadata":{"id":"e07e606d-930e-4371-ac83-ba2e7b4f19b2"},"source":["To check if Pandas has recognised the information in the Times column as date-time information we can check its type. We can access the information in this column directly by using the 'index' accessor, as we set this column as the index earlier:"]},{"cell_type":"code","execution_count":null,"id":"87e670d2-4aa5-4740-9308-15a94cff3720","metadata":{"id":"87e670d2-4aa5-4740-9308-15a94cff3720"},"outputs":[],"source":["type(obsdata.index)"]},{"cell_type":"markdown","id":"cd03f79f-0b3b-4583-8c8a-8d9fa0cc28ea","metadata":{"id":"cd03f79f-0b3b-4583-8c8a-8d9fa0cc28ea"},"source":["'DatetimeIndex' is showing up, which means that Pandas has indeed recognised it as a special category of data - date and time information."]},{"cell_type":"markdown","id":"b21602a0-1e65-4db6-ac45-e9d9ada3fd82","metadata":{"id":"b21602a0-1e65-4db6-ac45-e9d9ada3fd82"},"source":["We can therefore pull out some useful information from it, such as the year, month, day, but also less obvious information such as day-of-week, quarter/season, days-in-month..."]},{"cell_type":"code","execution_count":null,"id":"f7f7ee16-a73e-497b-8f92-c7d41aba1aa2","metadata":{"id":"f7f7ee16-a73e-497b-8f92-c7d41aba1aa2"},"outputs":[],"source":["obsdata.index.year"]},{"cell_type":"markdown","id":"24b20a7b-4b8c-4ff8-a926-f9e334efcdd1","metadata":{"id":"24b20a7b-4b8c-4ff8-a926-f9e334efcdd1"},"source":["We can use this information to group the rows of the table into categories, and then perform some operation on these groups. For example, to find the yearly mean for each catchment:"]},{"cell_type":"code","execution_count":null,"id":"0ef5f0c3-2c1d-4ee6-a83f-1fd152eca429","metadata":{"id":"0ef5f0c3-2c1d-4ee6-a83f-1fd152eca429"},"outputs":[],"source":["annavgs = obsdata.groupby(obsdata.index.year).mean()\n","\n","annavgs"]},{"cell_type":"markdown","id":"994bacd6-10a6-4a3a-9357-0e8945abdcb2","metadata":{"id":"994bacd6-10a6-4a3a-9357-0e8945abdcb2"},"source":["Note that the default behaviour of arithmetic operations (such as mean, sum etc.) applied to pandas dataframes where there is missing data is to ignore the missing data when doing the calculation. For example if a given catchment has a handful of observations that are showing as NaNs for a given year, the mean for that catchment-year will be calculated ignoring the NaN values completely. Only catchment-years where *all* the observations are NaN does a NaN get returned in an arithmetic calculation. This behaviour is customisable. Take a look at the [Pandas documentation](https://pandas.pydata.org/) for more information."]},{"cell_type":"markdown","id":"c69aa4d8-578e-4580-bae4-0602468aa44c","metadata":{"id":"c69aa4d8-578e-4580-bae4-0602468aa44c"},"source":["Pandas [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) logic is very powerful and useful, especially this ability to parse dates and times. This example is just one of many things that can be done with it. Another example could be to add a column describing a particular catchment property, like steepness or size, group the catchments into bins from smallest to largest or shallowest to steepest, and see how the mean flow varies with these properties."]},{"cell_type":"markdown","id":"18e57a99-1b4c-43f1-95d5-14c4efafd34f","metadata":{"id":"18e57a99-1b4c-43f1-95d5-14c4efafd34f"},"source":["Let's keep it simple and take a look at a couple of the annual averages we've got."]},{"cell_type":"code","execution_count":null,"id":"02683e1e-a551-4aa3-8417-263809c99e4a","metadata":{"id":"02683e1e-a551-4aa3-8417-263809c99e4a"},"outputs":[],"source":["annavgs['39001'].plot(label='Thames')\n","axes = plt.gca()\n","annavgs['23001'].plot(ax=axes, label='Tyne')\n","plt.legend()\n","\n","corr = np.corrcoef(annavgs['39001'], annavgs['23001'])[0,1] #correlation between the two time series\n","plt.title('R = ' + str(corr))"]},{"cell_type":"markdown","id":"d8f3e7cf-c3f0-4d9c-9079-bd7ed8739c48","metadata":{"id":"d8f3e7cf-c3f0-4d9c-9079-bd7ed8739c48"},"source":["The Tyne is a large river in the northeast of England. There is not as much correlation between the two rivers as you might think!"]},{"cell_type":"markdown","id":"25261a59-b115-4b76-9644-2e1854b8884b","metadata":{"id":"25261a59-b115-4b76-9644-2e1854b8884b"},"source":["Pandas can also group by meteorological season, which are defined based on the calendar months:\n","- Winter: December, January, February (DJF)\n","- Spring: March, April, May (MAM)\n","- Summer: June, July, August (JJA)\n","- Autumn: September, October, November (SON)\n","\n","We can create seasonal averages by making further use of pandas's date-time handling. Let's look at the 90th percentile of the winter flows in each year for the same two catchments above."]},{"cell_type":"markdown","id":"8dda8592-294a-442e-9183-ad2e5088c189","metadata":{"id":"8dda8592-294a-442e-9183-ad2e5088c189"},"source":["Note that because 'quarters starting in December' is not a property of the Index (unlike years), we have to use a more sophisticated method to group the rows the way we would like.\n","\n","Pandas has a [```Grouper```](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html) function that allows for more complicated groupings. To see what else it allows, you can run ```pd.Grouper?```."]},{"cell_type":"code","execution_count":null,"id":"8d2a99a5-1472-47fa-9a3b-acc36b6d9143","metadata":{"id":"8d2a99a5-1472-47fa-9a3b-acc36b6d9143"},"outputs":[],"source":["pc90 = obsdata.groupby(pd.Grouper(freq='QS-DEC')).quantile(0.9)\n","\n","pc90"]},{"cell_type":"markdown","id":"92482d76-9133-4a97-8b39-6abed5bc8faa","metadata":{"id":"92482d76-9133-4a97-8b39-6abed5bc8faa"},"source":["To select out only the winter flows, we want every 4th row, which we can represent as ```::4```. This is python indexing shorthand for ```0:-1:4``` which translates as the 0th (first) row to the -1th (last) row with a 'stride' (or step) of 4."]},{"cell_type":"markdown","id":"2495a897-829d-4e9a-a95c-cc6d16cef78a","metadata":{"id":"2495a897-829d-4e9a-a95c-cc6d16cef78a"},"source":["You'll see [```loc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) and [```iloc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) a lot with Pandas. These are powerful methods used to select out rows and columns from the table.[```loc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) can be used to select out cells or ranges of cells using the row and column *names* or *labels*, whereas [```iloc```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) is used to select based on the row and column *indices* (e.g. the cell at row 8 and column 5 would be selected with ```obsdata.iloc[8, 5]``` and ```obsdata.loc['1961-01-08', '04003']``` - try it out for yourself!)."]},{"cell_type":"code","execution_count":null,"id":"bd9d6026-0c0c-4837-8b4c-a97a53f585f5","metadata":{"id":"bd9d6026-0c0c-4837-8b4c-a97a53f585f5"},"outputs":[],"source":["pc90_DJF = pc90.iloc[::4, :]\n","pc90_DJF"]},{"cell_type":"code","execution_count":null,"id":"2228ad33-7d37-43b3-b7ad-40df87114947","metadata":{"id":"2228ad33-7d37-43b3-b7ad-40df87114947"},"outputs":[],"source":["thames = pc90_DJF['39001'].plot(label='thames')\n","axes = plt.gca()\n","tyne = pc90_DJF['23001'].plot(ax=axes, label='tyne')\n","plt.legend()\n","\n","corr = np.corrcoef(pc90_DJF['39001'], pc90_DJF['23001'])[0,1]\n","plt.title('R = ' + str(corr))"]},{"cell_type":"markdown","id":"7fae265f-4cb5-4e74-b082-8a90050562bf","metadata":{"id":"7fae265f-4cb5-4e74-b082-8a90050562bf"},"source":["## [ASCII Grid files (ESRI format)](https://en.wikipedia.org/wiki/Esri_grid)"]},{"cell_type":"markdown","id":"dca73fda-9143-43a9-a639-1cf91916c7f2","metadata":{"id":"dca73fda-9143-43a9-a639-1cf91916c7f2"},"source":["Another common file you might come across in hydrology is a file with the extension '**.grd**'. These files are often used as inputs/outputs of hydrological models. They are another textual representation of gridded data."]},{"cell_type":"markdown","id":"38120d14-b446-4917-8421-d127b10a7be6","metadata":{"id":"38120d14-b446-4917-8421-d127b10a7be6"},"source":["Ultimately these are just numbers laid out in a (usually) tab-separated pattern with some header lines at the top describing metadata needed to interpret the data into a grid. I find the easiest way of working with these is to read them into [Xarray](https://docs.xarray.dev/en/stable/), the now well established go-to library for working with gridded and N-dimensional data in various formats."]},{"cell_type":"markdown","id":"2dfa2094-2e43-4257-9b05-dbf71b54753a","metadata":{"id":"2dfa2094-2e43-4257-9b05-dbf71b54753a"},"source":["Here we'll develop the code to do this, but first let's take a look at what a typical grd file looks like if you open it in a text editor:"]},{"cell_type":"markdown","id":"e98cff9f-dbfe-4560-8f2b-a0d826de08e0","metadata":{"id":"e98cff9f-dbfe-4560-8f2b-a0d826de08e0"},"source":["**Bonus:** The next code cell also shows how to display an image stored in an object store"]},{"cell_type":"code","execution_count":null,"id":"d078a230-adf4-4ed2-874c-d44ecfea4fb1","metadata":{"id":"d078a230-adf4-4ed2-874c-d44ecfea4fb1"},"outputs":[],"source":["from PIL import Image\n","fs_img = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n","display(Image.open(fs_img.open('s3://example-data/grdfile.png')))"]},{"cell_type":"markdown","id":"b2c63f1e-7bba-475d-be1e-6195707516a2","metadata":{"id":"b2c63f1e-7bba-475d-be1e-6195707516a2"},"source":["Here you can clearly see the 6 header lines, followed by the rows & columns of numeric data."]},{"cell_type":"markdown","id":"ba46fbe5-af79-462e-a752-6bdac6153211","metadata":{"id":"ba46fbe5-af79-462e-a752-6bdac6153211"},"source":["- The **ncols** and **nrows** information is fairly self-explanatory\n","- The **xllcorner** and **yllcorner** are telling you what the x and y coordinates are of the 'lower left' corner of the grid. Important to note that these numbers represent the *corner* of the gridcell in the lower left corner of the grid, not it's centre point.\n","- **cellsize** tells you the x and y extent of the each cell in the grid, i.e. the distance between gridcells\n","- **NODATA_value** is also fairly self-explanatory"]},{"cell_type":"markdown","id":"33a6a4ac-c39a-4a2b-b25b-0fd0d48f4db0","metadata":{"id":"33a6a4ac-c39a-4a2b-b25b-0fd0d48f4db0"},"source":["These files are only able to represent regular grids (grids that have consistent spacing between the grid cells) that also have the same spacing in the x and y direction."]},{"cell_type":"markdown","id":"07d646c5-f873-49f5-a0be-fc852b8e896a","metadata":{"id":"07d646c5-f873-49f5-a0be-fc852b8e896a"},"source":["The header information contains everything we need to know in order to read this into Xarray."]},{"cell_type":"markdown","id":"b610042e-5501-4051-b869-1b0298517dbf","metadata":{"id":"b610042e-5501-4051-b869-1b0298517dbf"},"source":["First we need to read in the original file. Reading in any textual data in python is simple:"]},{"cell_type":"code","execution_count":null,"id":"0576be3e-f9a8-4f1f-a9d3-b10017e7b245","metadata":{"id":"0576be3e-f9a8-4f1f-a9d3-b10017e7b245"},"outputs":[],"source":["s3 = s3fs.S3FileSystem(anon=True, endpoint_url=\"https://fdri-o.s3-ext.jc.rl.ac.uk\")\n","grdfile = s3.open('s3://example-data/absw_230_2009_06.grd')"]},{"cell_type":"code","execution_count":null,"id":"07bc9221-88ad-4ad0-b542-9a97e0ef6668","metadata":{"id":"07bc9221-88ad-4ad0-b542-9a97e0ef6668"},"outputs":[],"source":["grdfile.readline()"]},{"cell_type":"markdown","id":"c317d23e-f880-42c3-89dc-aa3c321b3eb8","metadata":{"id":"c317d23e-f880-42c3-89dc-aa3c321b3eb8"},"source":["The [```readline()```](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) function reads the current line as a string, then the next time it is called, does the same for the next line. To grab the entire file contents at once, we can use ```readlines()```:"]},{"cell_type":"code","execution_count":null,"id":"38e3aa2a-981f-47e5-adac-85a34f216072","metadata":{"id":"38e3aa2a-981f-47e5-adac-85a34f216072"},"outputs":[],"source":["filecontents = grdfile.readlines()\n","\n","filecontents"]},{"cell_type":"markdown","id":"ccdaa08c-435e-4d0d-9211-4f15c5f29c7d","metadata":{"id":"ccdaa08c-435e-4d0d-9211-4f15c5f29c7d"},"source":["Each row becomes an item in the list"]},{"cell_type":"markdown","id":"143f2a95-0d33-4078-a3db-22685e90933c","metadata":{"id":"143f2a95-0d33-4078-a3db-22685e90933c"},"source":["**Note:** That we seem to missing the top row of the file. This is because we already called ```readline``` which advances the line number the next call to ```readline``` or ```readlines``` will read from. To read from the top of the file, the file needs to be reopened."]},{"cell_type":"markdown","id":"2d412c1e-a6a3-4589-ac0b-35b50b1df41b","metadata":{"id":"2d412c1e-a6a3-4589-ac0b-35b50b1df41b"},"source":["When we have finished working with a file using this method it is important to manually close it, otherwise it will remain open in memory and can cause all sorts of interesting errors and bugs, especially when loops are involved!"]},{"cell_type":"code","execution_count":null,"id":"62b9fa32-ce56-480d-a98b-92f5340e149c","metadata":{"id":"62b9fa32-ce56-480d-a98b-92f5340e149c"},"outputs":[],"source":["grdfile.close()"]},{"cell_type":"markdown","id":"c7e7ab0b-efcb-440e-a353-85537879995e","metadata":{"id":"c7e7ab0b-efcb-440e-a353-85537879995e"},"source":["We could read in the entire file this way, however for numeric data other libraries, mainly [Numpy](https://numpy.org/), tend to be easier to work with. But now that we know how to read in text, we can pull out the header information which we can then use to construct the grid."]},{"cell_type":"code","execution_count":null,"id":"dd3bc7e6-533f-4431-9ede-80110d0e0d53","metadata":{"id":"dd3bc7e6-533f-4431-9ede-80110d0e0d53"},"outputs":[],"source":["filein = 's3://example-data/absw_230_2009_06.grd'\n","grdfile = s3.open(filein)\n","ncols  = int(grdfile.readline().split()[1])\n","nrows  = int(grdfile.readline().split()[1])\n","xllc   = float(grdfile.readline().split()[1])\n","yllc   = float(grdfile.readline().split()[1])\n","res    = float(grdfile.readline().split()[1])\n","nodata = float(grdfile.readline().split()[1])\n","print('ncols: ' + str(ncols))\n","print('nrows: ' + str(nrows))\n","print('xllc: ' + str(xllc))\n","print('yllc: ' + str(yllc))\n","print('res: ' + str(res))\n","print('nodata: ' + str(nodata))\n","grdfile.close()"]},{"cell_type":"markdown","id":"cbd25d25-2090-441f-b91e-ab9ed2e9df35","metadata":{"id":"cbd25d25-2090-441f-b91e-ab9ed2e9df35"},"source":["Next we can use [Numpy's ```loadtxt```](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html) function to read in the actual numeric data, skipping the header lines (skip the first 6 rows):"]},{"cell_type":"code","execution_count":null,"id":"52ae7d94-e6ef-460e-b38b-ebc6b5824f21","metadata":{"id":"52ae7d94-e6ef-460e-b38b-ebc6b5824f21"},"outputs":[],"source":["grdfile = s3.open(filein)\n","data = np.loadtxt(grdfile,skiprows=6)\n","grdfile.close()"]},{"cell_type":"code","execution_count":null,"id":"c50b1683-7bc8-4aa1-95e6-0150f650029d","metadata":{"id":"c50b1683-7bc8-4aa1-95e6-0150f650029d"},"outputs":[],"source":["data"]},{"cell_type":"markdown","id":"045f0fab-ea5a-4550-b001-1a3ae10c8608","metadata":{"id":"045f0fab-ea5a-4550-b001-1a3ae10c8608"},"source":["Numpy has cleverly determined the data it is reading in is a grid, and stored it as such in an array"]},{"cell_type":"markdown","id":"bdecae37-5548-4907-b875-a1cd3dbafbc8","metadata":{"id":"bdecae37-5548-4907-b875-a1cd3dbafbc8"},"source":["Now all we need to do is generate the coordinates from the information we read in from the header:"]},{"cell_type":"markdown","id":"a2b64416-c30c-44b5-be3b-d13a681d8580","metadata":{"id":"a2b64416-c30c-44b5-be3b-d13a681d8580"},"source":["The [Numpy ```linspace```](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) command generates a sequence of equally spaced numbers: ```np.linspace(start, end, number of elements in sequence)```"]},{"cell_type":"markdown","id":"194ba763-4313-4256-a75c-c3da6e59b14c","metadata":{"id":"194ba763-4313-4256-a75c-c3da6e59b14c"},"source":["We can work out all of these from the header information. Remember that the xllc and yllc are the coordinates of the *corner* of the lower left gridcell, and generally gridcell coordinates are for the *centre* of each gridcell.\n","\n","Note the use of ```res/2```. This is because ```res``` defines the size of each grid cell and we want the center points so we have to move half the width and half the height from the bottom left corner."]},{"cell_type":"code","execution_count":null,"id":"bad7ca03-1ea8-4561-8a70-a31f74811118","metadata":{"id":"bad7ca03-1ea8-4561-8a70-a31f74811118"},"outputs":[],"source":["xcoords = np.linspace(xllc + (res/2), xllc + (res/2) + (res*(ncols-1)), ncols)\n","ycoords = np.linspace(yllc + (res/2), yllc + (res/2) + (res*(nrows-1)), nrows)"]},{"cell_type":"code","execution_count":null,"id":"2a323d4e-0bad-4fd6-a0a5-680bb6248729","metadata":{"scrolled":true,"id":"2a323d4e-0bad-4fd6-a0a5-680bb6248729"},"outputs":[],"source":["xcoords"]},{"cell_type":"code","execution_count":null,"id":"b3972c32-0918-450a-ba27-e68700ec1899","metadata":{"id":"b3972c32-0918-450a-ba27-e68700ec1899"},"outputs":[],"source":["ycoords"]},{"cell_type":"markdown","id":"9ace394e-3119-4cbe-8a6a-c0ed09ce7799","metadata":{"id":"9ace394e-3119-4cbe-8a6a-c0ed09ce7799"},"source":["Now we are ready for Xarray to take charge. Xarray requires 3 key pieces of information:\n","- The data values themselves\n","- The names of the dimensions/coordinates (in this case they can just be 'x' and 'y')\n","- The coordinates of each of the dimensions (in this case the x and y coordinates we've just generated)"]},{"cell_type":"code","execution_count":null,"id":"591c6722-65c0-40e7-bd65-7801c49e678d","metadata":{"id":"591c6722-65c0-40e7-bd65-7801c49e678d"},"outputs":[],"source":["ydimname = 'y'\n","xdimname = 'x'\n","grdxr = xr.DataArray(data, coords=[(ydimname, ycoords), (xdimname, xcoords)])\n","grdxr = grdxr.where(grdxr != nodata)"]},{"cell_type":"code","execution_count":null,"id":"2b03fb7f-5684-4684-861d-94eadbbc0b3b","metadata":{"id":"2b03fb7f-5684-4684-861d-94eadbbc0b3b"},"outputs":[],"source":["grdxr"]},{"cell_type":"markdown","id":"8bba09b2-eccc-4422-bd42-3c826b9e49b8","metadata":{"id":"8bba09b2-eccc-4422-bd42-3c826b9e49b8"},"source":["An Xarray DataArray is a coordinate-aware representation of our data, much like a version of Pandas designed for gridded and N-dimensional data, allowing operations and analyses to be carried out based on the *coordinate values*. We will learn more about Xarray soon!"]},{"cell_type":"markdown","id":"f8928c47-4f4d-469c-af8d-c29bf51f51ea","metadata":{"id":"f8928c47-4f4d-469c-af8d-c29bf51f51ea"},"source":["For now, let's plot the data and see what it looks like:"]},{"cell_type":"code","execution_count":null,"id":"6c825384-2004-4e35-bc64-de142b6d03ea","metadata":{"id":"6c825384-2004-4e35-bc64-de142b6d03ea"},"outputs":[],"source":["grdxr.plot()"]},{"cell_type":"markdown","id":"9237afc6-3aaf-4512-add5-006d7ad6b0ea","metadata":{"id":"9237afc6-3aaf-4512-add5-006d7ad6b0ea"},"source":["Oh dear, that appears to be upside down! How might we fix this?"]},{"cell_type":"code","execution_count":null,"id":"38425d3b-4cc3-4bd2-b41a-389aebbca117","metadata":{"id":"38425d3b-4cc3-4bd2-b41a-389aebbca117"},"outputs":[],"source":["xcoords = np.linspace(xllc + (res/2), xllc + (res/2) + (res*(ncols-1)), ncols)\n","ycoords = np.linspace(yllc + (res/2), yllc + (res/2) +  (res*(nrows-1)), nrows)[::-1] # using -1 inverts the y coordinates\n","\n","ydimname = 'y'\n","xdimname = 'x'\n","grdxr = xr.DataArray(data, coords=[(ydimname, ycoords), (xdimname, xcoords)])\n","grdxr = grdxr.where(grdxr != nodata)"]},{"cell_type":"code","execution_count":null,"id":"415edf81-7372-4692-944e-4d86125f2d64","metadata":{"id":"415edf81-7372-4692-944e-4d86125f2d64"},"outputs":[],"source":["grdxr.plot()"]},{"cell_type":"markdown","id":"4fc0c7dc-91db-4333-aabe-1b5a148bc367","metadata":{"id":"4fc0c7dc-91db-4333-aabe-1b5a148bc367"},"source":["Now we have a basic Xarray representation of the data, which comes with various powerful analysis tools, some of which we will explore in the third notebook, with data that is a little more interesting than all zeroes!"]},{"cell_type":"markdown","id":"592239dd-e372-43f2-b7c7-950c783d5914","metadata":{"id":"592239dd-e372-43f2-b7c7-950c783d5914"},"source":["## Further Resources"]},{"cell_type":"markdown","id":"f563eb92-a10b-4f5a-9183-0db8f165e86a","metadata":{"id":"f563eb92-a10b-4f5a-9183-0db8f165e86a"},"source":["The interested reader might want to check out [Polars](https://pola.rs/), which is a more efficient rewrite of pandas for improved speed in handling large tabular datasets and databases. The commands are largely the same as Pandas.\n","\n","- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html)\n","- [Numpy loadtxt documentation](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html)\n","- [Pandas groupby documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n","- [Pandas Grouper documentation](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html)\n","- [National River Flow Archive (NRFA) catchment information](https://nrfa.ceh.ac.uk/data/about-data/catchment-information)"]},{"cell_type":"code","execution_count":null,"id":"a9e281d4-53d1-4364-8340-0c006223f8ae","metadata":{"id":"a9e281d4-53d1-4364-8340-0c006223f8ae"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"zarrv3","language":"python","name":"zarrv3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}